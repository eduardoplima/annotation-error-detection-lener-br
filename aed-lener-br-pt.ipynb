{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardoplima/annotation-error-detection-lener-br/blob/main/aed-lener-br-pt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PC4E4Cm7rxn"
      },
      "source": [
        "# Detecção de erros de anotação no dataset LeNER-Br\n",
        "\n",
        "A detecção de erros de anotação (Annotation Error Detection) é uma técnica utilizada para identificar inconsistências ou rótulos incorretos em conjuntos de dados anotados manualmente. Esses erros podem comprometer a qualidade dos modelos treinados sobre esses dados, em tarefas como o reconhecimento de entidades nomeadas. Ferramentas e métodos de detecção buscam localizar essas falhas de forma automatizada, garantindo maior confiabilidade nos dados e melhor desempenho dos modelos.\n",
        "\n",
        "Nesse notebook analisaremos o dataset LeNER-Br com o objetivo de identificar possíveis erros de anotação utilizando a técnica de confident learning, implementada pela biblioteca Cleanlab. Essa abordagem permite detectar instâncias rotuladas incorretamente com base nas previsões probabilísticas de um classificador, como veremos no código abaixo.\n",
        "\n",
        "O dataset LeNER-Br é um corpus em português voltado para o reconhecimento de entidades nomeadas (NER) em textos jurídicos brasileiros. Desenvolvido por Luz et al. (2018), o LeNER-Br é composto exclusivamente por documentos legais, como decisões judiciais e pareceres, coletados de diversos tribunais brasileiros. Ele foi manualmente anotado para identificar entidades como pessoas, organizações, localizações e expressões temporais, além de categorias jurídicas específicas como LEGISLAÇÃO e JURISPRUDÊNCIA, que não são comuns em outros corpora do português. A descrição completa do trabalho pode ser lida no artigo disponível em https://teodecampos.github.io/LeNER-Br/luz_etal_propor2018.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGQ4_EVNU4vJ"
      },
      "source": [
        "# Configuração do ambiente\n",
        "\n",
        "Instalamos a biblioteca Cleanlab, que será empregada na aplicação de técnicas de confident learning para identificar possíveis erros de anotação no dataset LeNER-Br. Em seguida, importamos as bibliotecas necessárias para o restante da análise e realizamos o download dos arquivos de treinamento e teste diretamente do repositório oficial do LeNER-Br. Como os arquivos estão no formato CoNLL, que organiza os dados em colunas, é necessário convertê-los para o formato BIO (Beginning, Inside, Outside), amplamente utilizado em tarefas de reconhecimento de entidades nomeadas, para facilitar o processamento subsequente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2S3U3tmOYnt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea92e24-d041-436e-a014-1f202520318f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cleanlab in /usr/local/lib/python3.11/dist-packages (2.7.1)\n",
            "Requirement already satisfied: numpy~=1.22 in /usr/local/lib/python3.11/dist-packages (from cleanlab) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.11/dist-packages (from cleanlab) (1.6.1)\n",
            "Requirement already satisfied: tqdm>=4.53.0 in /usr/local/lib/python3.11/dist-packages (from cleanlab) (4.67.1)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from cleanlab) (2.2.2)\n",
            "Requirement already satisfied: termcolor>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from cleanlab) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->cleanlab) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->cleanlab) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->cleanlab) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->cleanlab) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->cleanlab) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->cleanlab) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->cleanlab) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install cleanlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zwf3b4wQNLzk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from cleanlab.filter import find_label_issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9rNT2HBeMtm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a565614-0e1c-4dfd-d8fa-724577d9dcee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-17 18:06:48--  https://raw.githubusercontent.com/eduardoplima/aed-lener-br/refs/heads/main/leNER-Br/train/train.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2142199 (2.0M) [text/plain]\n",
            "Saving to: ‘train.conll.4’\n",
            "\n",
            "train.conll.4       100%[===================>]   2.04M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-05-17 18:06:50 (215 MB/s) - ‘train.conll.4’ saved [2142199/2142199]\n",
            "\n",
            "--2025-05-17 18:06:50--  https://raw.githubusercontent.com/eduardoplima/aed-lener-br/refs/heads/main/leNER-Br/test/test.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 438441 (428K) [text/plain]\n",
            "Saving to: ‘test.conll.4’\n",
            "\n",
            "test.conll.4        100%[===================>] 428.17K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2025-05-17 18:06:51 (80.6 MB/s) - ‘test.conll.4’ saved [438441/438441]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/eduardoplima/aed-lener-br/refs/heads/main/leNER-Br/train/train.conll\n",
        "!wget https://raw.githubusercontent.com/eduardoplima/aed-lener-br/refs/heads/main/leNER-Br/test/test.conll\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ogVCKNIB6PyC"
      },
      "outputs": [],
      "source": [
        "NUM_FOLDS_CV = 5\n",
        "RANDOM_SEED = 43 # quase 42 😂 (e primo!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Oh7kICU9Y0rM"
      },
      "outputs": [],
      "source": [
        "def carregar_conll_lener(caminho_arquivo):\n",
        "    sentencas = []\n",
        "    tokens_sentenca_atual = []\n",
        "    etiquetas_sentenca_atual = []\n",
        "\n",
        "    with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
        "        for linha in f:\n",
        "          linha = linha.strip()\n",
        "          if linha:\n",
        "              partes = linha.split()\n",
        "              if len(partes) >= 2:\n",
        "                  token = partes[0]\n",
        "                  etiqueta_ner = partes[-1]\n",
        "                  tokens_sentenca_atual.append(token)\n",
        "                  etiquetas_sentenca_atual.append(etiqueta_ner)\n",
        "              else:\n",
        "                  pass\n",
        "          else:\n",
        "              if tokens_sentenca_atual:\n",
        "                  sentencas.append(list(zip(tokens_sentenca_atual, etiquetas_sentenca_atual)))\n",
        "                  tokens_sentenca_atual = []\n",
        "                  etiquetas_sentenca_atual = []\n",
        "\n",
        "        if tokens_sentenca_atual:\n",
        "            sentencas.append(list(zip(tokens_sentenca_atual, etiquetas_sentenca_atual)))\n",
        "\n",
        "    return sentencas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VLUrrOFwY8Ar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "160c0199-2239-45e3-ce54-905337d5bd9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Carregadas 7827 sentenças de train.conll.\n"
          ]
        }
      ],
      "source": [
        "sentencas_treino = carregar_conll_lener(\"train.conll\")\n",
        "print(f\"\\nCarregadas {len(sentencas_treino)} sentenças de train.conll.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lLHkMYcTlva"
      },
      "source": [
        "A seguir vemos alguns exemplo de sentenças em formato BIO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mEZezenubD0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d438b614-30f7-4c49-f36e-c4da61f567c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de sentença:\n",
            "V.v\tO\n",
            "APELAÇÃO\tO\n",
            "CÍVEL\tO\n",
            "-\tO\n",
            "NULIDADE\tO\n",
            "PROCESSUAL\tO\n",
            "-\tO\n",
            "INTIMAÇÃO\tO\n",
            "DO\tO\n",
            "MINISTÉRIO\tB-ORGANIZACAO\n",
            "PÚBLICO\tI-ORGANIZACAO\n",
            "-\tO\n",
            "INCAPAZ\tO\n",
            "ACOMPANHADA\tO\n",
            "DE\tO\n",
            "REPRESENTANTE\tO\n",
            "LEGAL\tO\n",
            "E\tO\n",
            "DE\tO\n",
            "ADVOGADO\tO\n",
            "-\tO\n",
            "EXERCÍCIO\tO\n",
            "DO\tO\n",
            "CONTRADITÓRIO\tO\n",
            "E\tO\n",
            "DA\tO\n",
            "AMPLA\tO\n",
            "DEFESA\tO\n",
            "-\tO\n",
            "AUSÊNCIA\tO\n",
            "DE\tO\n",
            "PREJUÍZOS\tO\n",
            "-\tO\n",
            "VÍCIO\tO\n",
            "AFASTADO\tO\n",
            "-\tO\n",
            "IMPROCEDÊNCIA\tO\n",
            "DO\tO\n",
            "PEDIDO\tO\n",
            "-\tO\n",
            "INEXISTÊNCIA\tO\n",
            "DE\tO\n",
            "PROVA\tO\n",
            "QUANTO\tO\n",
            "AO\tO\n",
            "FATO\tO\n",
            "CONSTITUTIVO\tO\n",
            "DO\tO\n",
            "DIREITO\tO\n",
            ".\tO\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nExemplo de sentença:\")\n",
        "for token, label in sentencas_treino[5][-500:]:\n",
        "    print(f\"{token}\\t{label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2PxuZmG0hUc"
      },
      "source": [
        "# Extração de tokens\n",
        "\n",
        "Conforme vimos, as sentenças extraídas estão organizadas em tuplas de tokens e seus respectivos labels BIO. O objetivo é extrair as features para cada token das nossas sentenças de treinamento utilizando um modelo BERT, via Hugging Face: nesse caso, o neuralmind/bert-large-portuguese-cased. Definimos explicitamente um tamanho máximo de 512, tamanho padrão do modelo BERT escolhido, para evitar OverflowError.\n",
        "\n",
        "O processo seguido em cada sentença é o seguinte: primeiro, extraímos os textos dos tokens. Em seguida, usamos o `tokenizer_hf` para converter esses textos em um formato que o modelo BERT entenda, especificando que a entrada já está dividida em palavras (is_split_into_words=True) e movendo os dados para o dispositivo de processamento (CPU ou GPU). Com os inputs prontos, passamos pelo model_hf para obter os \"hidden states\" da última camada, que são os embeddings contextuais para cada subpalavra gerada pelo tokenizador. Como o BERT trabalha com subpalavras, precisamos alinhar esses embeddings de volta aos nossos tokens originais. Para isso, utilizamos os word_ids fornecidos pelo tokenizador e, para cada token original, calculamos a média dos embeddings de suas subpalavras constituintes. Esses vetores médios são então adicionados à nossa lista final features_tokens, representando numericamente cada palavra do nosso corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uuhNSeqc5SJo"
      },
      "outputs": [],
      "source": [
        "hf_model_name = \"neuralmind/bert-large-portuguese-cased\"\n",
        "\n",
        "tokenizer_hf = AutoTokenizer.from_pretrained(hf_model_name)\n",
        "model_hf = AutoModel.from_pretrained(hf_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0Hdwt7Uau6N",
        "outputId": "bb61dacf-d4a1-477c-c887-08f14e22d7d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total de tokens nos dados de treinamento: 229277\n",
            "Labels únicas do LeNER-Br: ['B-JURISPRUDENCIA', 'B-LEGISLACAO', 'B-LOCAL', 'B-ORGANIZACAO', 'B-PESSOA', 'B-TEMPO', 'I-JURISPRUDENCIA', 'I-LEGISLACAO', 'I-LOCAL', 'I-ORGANIZACAO', 'I-PESSOA', 'I-TEMPO', 'O']\n"
          ]
        }
      ],
      "source": [
        "todos_tokens = []\n",
        "todos_labels_ner = []\n",
        "ids_sentenca = []\n",
        "\n",
        "for i, sentenca in enumerate(sentencas_treino):\n",
        "  for token_text, ner_tag in sentenca:\n",
        "    todos_tokens.append(token_text)\n",
        "    todos_labels_ner.append(ner_tag)\n",
        "    ids_sentenca.append(i)\n",
        "\n",
        "print(f\"\\nTotal de tokens nos dados de treinamento: {len(todos_tokens)}\")\n",
        "print(f\"Labels únicas do LeNER-Br: {sorted(list(set(todos_labels_ner)))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temos um total de 13 labels únicas no nosso dataset. A seguir vemos uma explicação do significado de cada uma delas:\n",
        "\n",
        "* **`B-JURISPRUDENCIA`**:\n",
        "    * **B**: Indica que este token é o **início** (Beginning) de uma entidade nomeada.\n",
        "    * **JURISPRUDENCIA**: Indica que a entidade nomeada é do tipo \"Jurisprudência\". Refere-se a decisões judiciais, acórdãos, súmulas ou qualquer conjunto de interpretações das leis feitas pelos tribunais.\n",
        "    * *Exemplo*: No texto \"Conforme o **Acórdão** nº 123...\", \"Acórdão\" poderia ser `B-JURISPRUDENCIA`.\n",
        "\n",
        "* **`B-LEGISLACAO`**:\n",
        "    * **B**: Início da entidade.\n",
        "    * **LEGISLACAO**: Indica que a entidade nomeada é do tipo \"Legislação\". Refere-se a leis, decretos, portarias, códigos, constituições, etc.\n",
        "    * *Exemplo*: No texto \"A **Lei** nº 8.666/93...\", \"Lei\" poderia ser `B-LEGISLACAO`.\n",
        "\n",
        "* **`B-LOCAL`**:\n",
        "    * **B**: Início da entidade.\n",
        "    * **LOCAL**: Indica que a entidade nomeada é um \"Local\". Pode ser uma cidade, estado, país, endereço, acidente geográfico, etc.\n",
        "    * *Exemplo*: No texto \"Ele viajou para **Paris**...\", \"Paris\" seria `B-LOCAL`.\n",
        "\n",
        "* **`B-ORGANIZACAO`**:\n",
        "    * **B**: Início da entidade.\n",
        "    * **ORGANIZACAO**: Indica que a entidade nomeada é uma \"Organização\". Inclui empresas, instituições governamentais, ONGs, times esportivos, etc.\n",
        "    * *Exemplo*: No texto \"O **Google** anunciou...\", \"Google\" seria `B-ORGANIZACAO`.\n",
        "\n",
        "* **`B-PESSOA`**:\n",
        "    * **B**: Início da entidade.\n",
        "    * **PESSOA**: Indica que a entidade nomeada é uma \"Pessoa\". Refere-se a nomes de indivíduos.\n",
        "    * *Exemplo*: No texto \"**Maria** Silva é advogada...\", \"Maria\" seria `B-PESSOA`.\n",
        "\n",
        "* **`B-TEMPO`**:\n",
        "    * **B**: Início da entidade.\n",
        "    * **TEMPO**: Indica que a entidade nomeada é uma referência temporal. Pode ser uma data, hora, período específico (ex: \"século XXI\", \"próxima semana\").\n",
        "    * *Exemplo*: No texto \"A reunião será em **15 de maio**...\", \"15\" poderia ser `B-TEMPO`.\n",
        "\n",
        "* **`I-JURISPRUDENCIA`**:\n",
        "    * **I**: Indica que este token está **dentro** (Inside) de uma entidade do tipo \"Jurisprudência\" que já começou. É uma continuação da entidade.\n",
        "    * *Exemplo*: No texto \"...o **Superior Tribunal** de Justiça...\", se \"Superior\" foi `B-JURISPRUDENCIA` (ou `B-ORGANIZACAO` dependendo do esquema), \"Tribunal\" poderia ser `I-JURISPRUDENCIA` (ou `I-ORGANIZACAO`). No caso de um nome de jurisprudência longo, como \"Súmula **Vinculante nº** 56\", \"Vinculante\", \"nº\" e \"56\" seriam `I-JURISPRUDENCIA` se \"Súmula\" fosse `B-JURISPRUDENCIA`.\n",
        "\n",
        "* **`I-LEGISLACAO`**:\n",
        "    * **I**: Dentro de uma entidade do tipo \"Legislação\".\n",
        "    * *Exemplo*: No texto \"A **Lei de Licitações**...\", se \"Lei\" foi `B-LEGISLACAO`, \"de\" e \"Licitações\" seriam `I-LEGISLACAO`.\n",
        "\n",
        "* **`I-LOCAL`**:\n",
        "    * **I**: Dentro de uma entidade do tipo \"Local\".\n",
        "    * *Exemplo*: No texto \"Ele mora em **Nova York**...\", se \"Nova\" foi `B-LOCAL`, \"York\" seria `I-LOCAL`.\n",
        "\n",
        "* **`I-ORGANIZACAO`**:\n",
        "    * **I**: Dentro de uma entidade do tipo \"Organização\".\n",
        "    * *Exemplo*: No texto \"O **Banco Central** do Brasil...\", se \"Banco\" foi `B-ORGANIZACAO`, \"Central\" seria `I-ORGANIZACAO`.\n",
        "\n",
        "* **`I-PESSOA`**:\n",
        "    * **I**: Dentro de uma entidade do tipo \"Pessoa\".\n",
        "    * *Exemplo*: No texto \"**Maria Joaquina** da Silva...\", se \"Maria\" foi `B-PESSOA`, \"Joaquina\" seria `I-PESSOA`.\n",
        "\n",
        "* **`I-TEMPO`**:\n",
        "    * **I**: Dentro de uma entidade do tipo \"Tempo\".\n",
        "    * *Exemplo*: No texto \"A reunião será em **15 de maio de 2025**...\", se \"15\" foi `B-TEMPO`, \"de\", \"maio\", \"de\" e \"2025\" seriam `I-TEMPO`.\n",
        "\n",
        "* **`O`**:\n",
        "    * **O**: Indica que o token está **fora** (Outside) de qualquer entidade nomeada. É um token comum que não faz parte de uma categoria de interesse específica.\n",
        "    * *Exemplo*: No texto \"O gato **sentou** no tapete.\", \"sentou\" seria `O`.\n"
      ],
      "metadata": {
        "id": "QYIvrgZpnSTf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx7eBlPHbxmf",
        "outputId": "16c525a4-5a96-4252-d5a9-1f5512f101ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando device: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando device: {device}\")\n",
        "\n",
        "EFFECTIVE_MAX_LENGTH = 512\n",
        "\n",
        "model_hf.to(device)\n",
        "model_hf.eval()\n",
        "\n",
        "features_tokens = []\n",
        "\n",
        "for i_sent, dados_sentenca in enumerate(sentencas_treino):\n",
        "    textos_sentenca = [dado_token[0] for dado_token in dados_sentenca]\n",
        "\n",
        "    if not textos_sentenca:\n",
        "        # Sentença vazia, pulamos\n",
        "        continue\n",
        "\n",
        "    inputs = tokenizer_hf(\n",
        "        textos_sentenca,\n",
        "        is_split_into_words=True,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"longest\",\n",
        "        truncation=True,\n",
        "        max_length=EFFECTIVE_MAX_LENGTH\n",
        "    ).to(device)\n",
        "\n",
        "    word_ids = inputs.word_ids()\n",
        "    with torch.no_grad():\n",
        "        outputs = model_hf(**inputs)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "    token_subword_embeddings = [[] for _ in range(len(textos_sentenca))]\n",
        "\n",
        "    for subword_idx, original_word_idx in enumerate(word_ids):\n",
        "        if original_word_idx is not None: # Exclui tokens como [CLS], [SEP] ou tokens de padding\n",
        "            embedding = last_hidden_state[0, subword_idx, :]\n",
        "            token_subword_embeddings[original_word_idx].append(embedding)\n",
        "\n",
        "    sentence_token_features = []\n",
        "    for original_token_idx in range(len(textos_sentenca)):\n",
        "        if token_subword_embeddings[original_token_idx]:\n",
        "            stacked_embeddings = torch.stack(token_subword_embeddings[original_token_idx])\n",
        "            mean_embedding = torch.mean(stacked_embeddings, dim=0)\n",
        "            sentence_token_features.append(mean_embedding.cpu().numpy())\n",
        "        else:\n",
        "            sentence_token_features.append(np.zeros(model_hf.config.hidden_size))\n",
        "\n",
        "    features_tokens.extend(sentence_token_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BilVsoQncfZi",
        "outputId": "d838789f-8864-43ec-bd62-f84ca876e259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formato da matriz de características dos tokens: (229277, 1024)\n"
          ]
        }
      ],
      "source": [
        "features_tokens = np.array(features_tokens)\n",
        "print(f\"Formato da matriz de características dos tokens: {features_tokens.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxJ31i-urTjS"
      },
      "source": [
        "# Treino do modelo\n",
        "\n",
        "Precisamos treinar um modelo para uso na biblioteca cleanlab. Inicialmente, os labels NER são transformados utilizando a LabelEncoder, que transforma os rótulos em números para uso no modelo de Regressão Logística escolhido.\n",
        "\n",
        "No efetivo treino do nosso modelo dividimos nosso conjunto de dados (`features_tokens` e `labels_ner_codificados`) em duas partes: uma para treinamento (`X_treino`,`y_treino`) e outra para teste (`X_teste`, `y_teste`). Utilizamos 25% dos dados para o conjunto de teste e garantimos que a proporção das diferentes classes de rótulos NER seja mantida em ambas as divisões, graças ao parâmetro stratify. Em seguida, preparamos um array chamado `probabilidades_preditas_teste`, que servirá para armazenar as probabilidades de cada classe que o nosso modelo atribuirá aos exemplos do conjunto de teste.\n",
        "\n",
        "Em seguida, definimos e treinamos o nosso modelo de classificação. Optamos por um SGDClassifier (Stochastic Gradient Descent Classifier). Ele funciona ajustando os parâmetros de um modelo linear (neste caso, configurado para se comportar como uma Regressão Logística usando loss='log_loss') de forma iterativa, processando uma amostra por vez, fazendo com que seja rápido e escalável. Após treinar o modelo, nós o utilizamos para prever as probabilidades das classes para o conjunto `X_teste`, armazenando-as em `probabilidades_preditas_teste`. Por fim, também calculamos e exibimos a acurácia do modelo nesse conjunto de teste, comparando as predições com os rótulos verdadeiros y_teste.\n",
        "\n",
        "Uma estratégia de KFold com 5 folds foi utilizada para percorrer o dataset inteiro de modo separado e independente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EKa34kR3g95n"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "labels_ner_codificados = label_encoder.fit_transform(todos_labels_ner)\n",
        "num_classes = len(label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo0f5tOBhCC4",
        "outputId": "53a0b39c-e825-4601-ae40-c6604c4a15b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando validação cruzada de 5 folds\n",
            "  Processando Fold 1/5...\n",
            "Modelo treinado.\n",
            "    Acurácia do Fold 1: 0.9680\n",
            "  Processando Fold 2/5...\n",
            "Modelo treinado.\n",
            "    Acurácia do Fold 2: 0.9734\n",
            "  Processando Fold 3/5...\n",
            "Modelo treinado.\n",
            "    Acurácia do Fold 3: 0.9696\n",
            "  Processando Fold 4/5...\n",
            "Modelo treinado.\n",
            "    Acurácia do Fold 4: 0.9720\n",
            "  Processando Fold 5/5...\n",
            "Modelo treinado.\n",
            "    Acurácia do Fold 5: 0.9719\n",
            "\n",
            "Coleta de probabilidades preditas fora da amostra finalizada.\n",
            "Formato da matriz de probabilidades_preditas: (229277, 13)\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=NUM_FOLDS_CV, shuffle=True, random_state=RANDOM_SEED)\n",
        "probabilidades_preditas = np.zeros((len(features_tokens), num_classes))\n",
        "print(f\"\\nIniciando validação cruzada de {NUM_FOLDS_CV} folds\")\n",
        "\n",
        "for indice_fold, (indices_treino, indices_validacao) in enumerate(skf.split(features_tokens, labels_ner_codificados)):\n",
        "    print(f\"  Processando Fold {indice_fold + 1}/{NUM_FOLDS_CV}...\")\n",
        "    X_treino, X_validacao = features_tokens[indices_treino], features_tokens[indices_validacao]\n",
        "    y_treino, y_validacao = labels_ner_codificados[indices_treino], labels_ner_codificados[indices_validacao]\n",
        "\n",
        "    modelo = SGDClassifier(\n",
        "            loss='log_loss',\n",
        "            penalty='l2',\n",
        "            alpha=0.0001,\n",
        "            max_iter=1000,\n",
        "            tol=1e-3,\n",
        "            random_state=RANDOM_SEED,\n",
        "            class_weight='balanced',\n",
        "            learning_rate='optimal',\n",
        "            early_stopping=True,\n",
        "            n_iter_no_change=10,\n",
        "            validation_fraction=0.1\n",
        "        )\n",
        "\n",
        "\n",
        "    modelo.fit(X_treino, y_treino)\n",
        "    print(\"Modelo treinado.\")\n",
        "    probabilidades_preditas_fold = modelo.predict_proba(X_validacao)\n",
        "    probabilidades_preditas[indices_validacao] = probabilidades_preditas_fold\n",
        "\n",
        "    predicoes_fold = modelo.predict(X_validacao)\n",
        "    acuracia_fold = accuracy_score(y_validacao, predicoes_fold)\n",
        "    print(f\"    Acurácia do Fold {indice_fold + 1}: {acuracia_fold:.4f}\")\n",
        "\n",
        "print(\"\\nColeta de probabilidades preditas fora da amostra finalizada.\")\n",
        "print(f\"Formato da matriz de probabilidades_preditas: {probabilidades_preditas.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xu4qFf13xNY"
      },
      "source": [
        "# Problemas encontrados com o dataset\n",
        "\n",
        "Agora adentramos na aplicação de fato das técnicas de confident learning. Inicialmente, utilizamos a função `find_label_issues` da biblioteca cleanlab para identificar tokens potencialmente mal rotulados em nosso dataset de NER. Passamos como entrada os label codificados (`labels_ner_codificados`) e as probabilidades previstas pelo modelo (`probabilidades_preditas`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels_ner_codificados),len(probabilidades_preditas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7-5LWpzcv2O",
        "outputId": "9d8df352-6694-4db7-d50a-74b6b8ee6bca"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(229277, 229277)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YcEe1bHWiMzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eff8717-6ff4-402c-8139-53bbab2f5cc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Identificando problemas de rotulagem com cleanlab...\n",
            "Cleanlab identificou 2326 potenciais problemas de rotulagem.\n",
            "Isso representa 1.01% do total de tokens.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nIdentificando problemas de rotulagem com cleanlab...\")\n",
        "\n",
        "indices_problemas_rotulos = find_label_issues(\n",
        "        labels=labels_ner_codificados,\n",
        "        pred_probs=probabilidades_preditas,\n",
        "        return_indices_ranked_by='self_confidence'\n",
        "    )\n",
        "\n",
        "num_problemas_encontrados = len(indices_problemas_rotulos)\n",
        "print(f\"Cleanlab identificou {num_problemas_encontrados} potenciais problemas de rotulagem.\")\n",
        "percentual_problemas = (num_problemas_encontrados / len(todos_tokens)) * 100\n",
        "print(f\"Isso representa {percentual_problemas:.2f}% do total de tokens.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AkTKFjaN84c"
      },
      "source": [
        "\n",
        "Em seguida, percorremos os índices dos tokens que apresentam possíveis erros de anotação no dataset de NER, comparando os rótulos originais com os rótulos sugeridos pelo modelo. Para cada token identificado como problemático, recuperamos o token, o label e o transformamos para sua forma textual usando o label_encoder (método `inverse_transform`).\n",
        "\n",
        "Entção, identificamos o rótulo predito pelo modelo com maior probabilidade e também o decodificamos. Calculamos a confiança do modelo no rótulo original e recuperamos o identificador da sentença a que o token pertence. Por fim, reunimos todas essas informações em uma lista de `dicts` (`problemas_para_revisao`).\n",
        "\n",
        "O `dict`  armazenado possui os seguintes campos que serão úteis para nossa análise posterior:\n",
        "\n",
        "* `indice_token_global`: posição do token em nossa lista com todos os tokens do nosso dataset\n",
        "* `id_sentenca`: identificador da frase problemática\n",
        "* `rotulo_original`: o label que veio associado ao token no dataset\n",
        "* `rotulo_sugerido_pelo_modelo`: o label que nosso modelo sugere para o token\n",
        "* `confianca_modelo_no_rotulo_original`: a probabilidade que o modelo atribui ao rótulo original. Valores baixos significam que nosso modelo não tem muita confiança que o label original está correto.\n",
        "* `contexto_sentenca_completa`: sentença completa onde o token problemático foi encontrado. Será utilizado para visualizarmos os problemas que serão tratados em um passo posterior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "L8eNCoDMiPRU"
      },
      "outputs": [],
      "source": [
        "problemas_para_revisao = []\n",
        "for indice_token_global in indices_problemas_rotulos:\n",
        "    token_original = todos_tokens[indice_token_global]\n",
        "    rotulo_original_codificado = labels_ner_codificados[indice_token_global]\n",
        "    rotulo_original_str = label_encoder.inverse_transform([rotulo_original_codificado])[0]\n",
        "    rotulo_predito_codificado = np.argmax(probabilidades_preditas[indice_token_global])\n",
        "    rotulo_predito_str = label_encoder.inverse_transform([rotulo_predito_codificado])[0]\n",
        "\n",
        "    confianca_no_original = probabilidades_preditas[indice_token_global, rotulo_original_codificado]\n",
        "\n",
        "    id_sent = ids_sentenca[indice_token_global]\n",
        "\n",
        "    problemas_para_revisao.append({\n",
        "        \"indice_token_global\": indice_token_global,\n",
        "        \"id_sentenca\": id_sent,\n",
        "        \"token\": token_original,\n",
        "        \"rotulo_original\": rotulo_original_str,\n",
        "        \"rotulo_sugerido_pelo_modelo\": rotulo_predito_str,\n",
        "        \"confianca_modelo_no_rotulo_original\": confianca_no_original,\n",
        "        \"contexto_sentenca_completa\": sentencas_treino[id_sent]\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfgj-RnWJB_U"
      },
      "source": [
        "Ordenamos os problemas pelas menores confianças do modelo nos rótulos fornecidos originalmente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "haGoZq34xJUa"
      },
      "outputs": [],
      "source": [
        "problemas_para_revisao_ordenados = sorted(problemas_para_revisao, key=lambda x: x['confianca_modelo_no_rotulo_original'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AMNDORdn3DF"
      },
      "source": [
        "E visualizamos os problemas encontrados. No laço a seguir temos os 20 problemas com menor confianca do modelo no rótulo original, ou seja, maior desconfiança."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ow2pyxdjzSEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264eadc1-138e-44d8-af5b-b7164e462ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problema #1 (Índice Global do Token: 138519)\n",
            "  ID da Sentença: 4905\n",
            "  Token: 'artigo'\n",
            "  Rótulo Original: B-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: B-LEGISLACAO\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): Logo(O) ,(O) tem-se(O) que(O) o(O) **artigo**(Original:B-LOCAL|Sugerido:B-LEGISLACAO)** 276(I-LOCAL) do(I-LOCAL) Decreto(I-LOCAL) nº(I-LOCAL) 3.048/99(I-LOCAL) especificamente(O) fixa(O) o(O) dia(O) dois(O)\n",
            "\n",
            "Problema #2 (Índice Global do Token: 122878)\n",
            "  ID da Sentença: 4323\n",
            "  Token: 'Autos'\n",
            "  Rótulo Original: B-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: B-JURISPRUDENCIA\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): 68(O) 3302-0444/0445(O) ,(O) Rio(B-LOCAL) Branco-AC(I-LOCAL) -(O) Mod(O) .(O) 500258(O) -(O) **Autos**(Original:B-LOCAL|Sugerido:B-JURISPRUDENCIA)** n.º(I-LOCAL) 1002199-81.2017.8.01.0000/50000(I-LOCAL) ARAÚJO(O) ,(O) QUARTA(B-ORGANIZACAO) TURMA(I-ORGANIZACAO) ,(O) julgado(O) em(O) 09/05/2017(B-TEMPO)\n",
            "\n",
            "Problema #3 (Índice Global do Token: 33548)\n",
            "  ID da Sentença: 1067\n",
            "  Token: ','\n",
            "  Rótulo Original: I-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: O\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): CONSTRUÇÃO(O) E(O) ALTERA(O) O(O) USO(O) DE(O) LOTES(O) NA(O) QUADRA(B-LOCAL) 1(I-LOCAL) **,**(Original:I-LOCAL|Sugerido:O)** DO(I-LOCAL) SETOR(I-LOCAL) DE(I-LOCAL) INDÚSTRIAS(I-LOCAL) GRÁFICAS(I-LOCAL) ,(O) NA(O) REGIÃO(B-LOCAL) ADMINISTRATIVA(I-LOCAL) DO(I-LOCAL)\n",
            "\n",
            "Problema #4 (Índice Global do Token: 122879)\n",
            "  ID da Sentença: 4323\n",
            "  Token: 'n.º'\n",
            "  Rótulo Original: I-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: I-JURISPRUDENCIA\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): 3302-0444/0445(O) ,(O) Rio(B-LOCAL) Branco-AC(I-LOCAL) -(O) Mod(O) .(O) 500258(O) -(O) Autos(B-LOCAL) **n.º**(Original:I-LOCAL|Sugerido:I-JURISPRUDENCIA)** 1002199-81.2017.8.01.0000/50000(I-LOCAL) ARAÚJO(O) ,(O) QUARTA(B-ORGANIZACAO) TURMA(I-ORGANIZACAO) ,(O) julgado(O) em(O) 09/05/2017(B-TEMPO) ,(O)\n",
            "\n",
            "Problema #5 (Índice Global do Token: 56502)\n",
            "  ID da Sentença: 1863\n",
            "  Token: 'TAPE'\n",
            "  Rótulo Original: B-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: O\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): COMPROMISSO(O) É(O) COM(O) A(O) VERDADE(O) POR(O) ISSO(O) O(O) PSDB(B-ORGANIZACAO) 1(O) **TAPE**(Original:B-LOCAL|Sugerido:O)** VI(I-LOCAL) APOIA(O) IGOR(B-PESSOA) E(O) TECO(B-PESSOA) .(O)\n",
            "\n",
            "Problema #6 (Índice Global do Token: 138520)\n",
            "  ID da Sentença: 4905\n",
            "  Token: '276'\n",
            "  Rótulo Original: I-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: I-LEGISLACAO\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): Logo(O) ,(O) tem-se(O) que(O) o(O) artigo(B-LOCAL) **276**(Original:I-LOCAL|Sugerido:I-LEGISLACAO)** do(I-LOCAL) Decreto(I-LOCAL) nº(I-LOCAL) 3.048/99(I-LOCAL) especificamente(O) fixa(O) o(O) dia(O) dois(O) do(O)\n",
            "\n",
            "Problema #7 (Índice Global do Token: 173708)\n",
            "  ID da Sentença: 6020\n",
            "  Token: 'Penal'\n",
            "  Rótulo Original: I-TEMPO\n",
            "  Rótulo Sugerido pelo Modelo: I-LEGISLACAO\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): 2.848(I-LEGISLACAO) ,(O) de(O) 7(B-TEMPO) de(I-TEMPO) dezembro(I-TEMPO) de(I-TEMPO) 1940(I-TEMPO) ((O) Código(B-TEMPO) **Penal**(Original:I-TEMPO|Sugerido:I-LEGISLACAO)** )(O) ,(O) passa(O) a(O) vigorar(O) com(O) as(O) seguintes(O) alterações(O) :(O)\n",
            "\n",
            "Problema #8 (Índice Global do Token: 45419)\n",
            "  ID da Sentença: 1482\n",
            "  Token: 'única'\n",
            "  Rótulo Original: I-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: I-ORGANIZACAO\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): fls(O) .(O) 23-TJ(O) pelo(O) douto(O) Juiz(O) de(O) Direito(O) da(O) Vara(B-LOCAL) **única**(Original:I-LOCAL|Sugerido:I-ORGANIZACAO)** da(I-LOCAL) Comarca(I-LOCAL) de(I-LOCAL) Santa(I-LOCAL) Maria(I-LOCAL) do(I-LOCAL) Suaçuí/MG(I-LOCAL) ,(O) que(O) indeferiu(O)\n",
            "\n",
            "Problema #9 (Índice Global do Token: 122880)\n",
            "  ID da Sentença: 4323\n",
            "  Token: '1002199-81.2017.8.01.0000/50000'\n",
            "  Rótulo Original: I-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: I-JURISPRUDENCIA\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): ,(O) Rio(B-LOCAL) Branco-AC(I-LOCAL) -(O) Mod(O) .(O) 500258(O) -(O) Autos(B-LOCAL) n.º(I-LOCAL) **1002199-81.2017.8.01.0000/50000**(Original:I-LOCAL|Sugerido:I-JURISPRUDENCIA)** ARAÚJO(O) ,(O) QUARTA(B-ORGANIZACAO) TURMA(I-ORGANIZACAO) ,(O) julgado(O) em(O) 09/05/2017(B-TEMPO) ,(O) DJe(O)\n",
            "\n",
            "Problema #10 (Índice Global do Token: 28356)\n",
            "  ID da Sentença: 943\n",
            "  Token: 'Estrada'\n",
            "  Rótulo Original: I-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: B-LOCAL\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): conformidade(O) com(O) as(O) seguintes(O) especificações(O) :(O) I(O) localização(O) :(O) DF-001(B-LOCAL) **Estrada**(Original:I-LOCAL|Sugerido:B-LOCAL)** Parque(I-LOCAL) Contorno(I-LOCAL) EPCT(I-LOCAL) ,(O) km(O) 12,8(O) ,(O) na(O) Região(B-LOCAL) Administrativa(I-LOCAL)\n",
            "\n",
            "Problema #11 (Índice Global do Token: 57701)\n",
            "  ID da Sentença: 1912\n",
            "  Token: 'Ceará'\n",
            "  Rótulo Original: B-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: I-ORGANIZACAO\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): ementas(O) de(O) julgados(O) dos(O) TREs(O) de(O) Minas(B-LOCAL) Gerais(I-LOCAL) e(O) do(O) **Ceará**(Original:B-LOCAL|Sugerido:I-ORGANIZACAO)** .(O)\n",
            "\n",
            "Problema #12 (Índice Global do Token: 54465)\n",
            "  ID da Sentença: 1795\n",
            "  Token: '27/02/2015'\n",
            "  Rótulo Original: B-PESSOA\n",
            "  Rótulo Sugerido pelo Modelo: B-TEMPO\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): MOURA(I-PESSOA) ,(O) SEXTA(B-ORGANIZACAO) TURMA(I-ORGANIZACAO) ,(O) julgado(O) em(O) 09/12/2014(B-TEMPO) ,(O) DJe(O) **27/02/2015**(Original:B-PESSOA|Sugerido:B-TEMPO)** ;(O) HC(B-JURISPRUDENCIA) 312.391/SP(I-JURISPRUDENCIA) ,(O) Rel(O) .(O) Ministro(O) FELIX(B-PESSOA) FISCHER(I-PESSOA) ,(O)\n",
            "\n",
            "Problema #13 (Índice Global do Token: 96444)\n",
            "  ID da Sentença: 3240\n",
            "  Token: 'artigo'\n",
            "  Rótulo Original: B-ORGANIZACAO\n",
            "  Rótulo Sugerido pelo Modelo: B-LEGISLACAO\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): não(O) concretização(O) ,(O) pelo(O) paciente(O) ,(O) do(O) tipo(O) previsto(O) no(O) **artigo**(Original:B-ORGANIZACAO|Sugerido:B-LEGISLACAO)** 187(I-ORGANIZACAO) do(I-ORGANIZACAO) Código(I-ORGANIZACAO) Penal(I-ORGANIZACAO) Militar(I-ORGANIZACAO) -(O) Crime(O) de(O) Deserção(O) -(O)\n",
            "\n",
            "Problema #14 (Índice Global do Token: 9642)\n",
            "  ID da Sentença: 341\n",
            "  Token: 'período'\n",
            "  Rótulo Original: B-TEMPO\n",
            "  Rótulo Sugerido pelo Modelo: O\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): $(O) 30.000,00(O) ((O) trinta(O) mil(O) reais(O) )(O) ,(O) alusivos(O) o(O) **período**(Original:B-TEMPO|Sugerido:O)** de(I-TEMPO) maio(I-TEMPO) a(I-TEMPO) novembro(I-TEMPO) de(I-TEMPO) 2014(I-TEMPO) ;(O) c(O) )(O) R(O)\n",
            "\n",
            "Problema #15 (Índice Global do Token: 33464)\n",
            "  ID da Sentença: 1066\n",
            "  Token: 'Federal.Por'\n",
            "  Rótulo Original: I-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: O\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): iniciativa(O) do(O) processo(O) legislativo(O) compete(O) privativamente(O) ao(O) Governador(O) do(O) Distrito(B-LOCAL) **Federal.Por**(Original:I-LOCAL|Sugerido:O)** isso(O) mesmo(O) ,(O) demonstrado(O) que(O) a(O) iniciativa(O) das(O) leis(O) distritais(O)\n",
            "\n",
            "Problema #16 (Índice Global do Token: 84104)\n",
            "  ID da Sentença: 2801\n",
            "  Token: 'Porta-Aviões'\n",
            "  Rótulo Original: B-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: I-LOCAL\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): ((O) ...(O) )(O) que(O) conheceu(O) o(O) Acusado(O) quando(O) serviu(O) no(O) **Porta-Aviões**(Original:B-LOCAL|Sugerido:I-LOCAL)** São(I-LOCAL) Paulo(I-LOCAL) ;(O) ((O) ...(O) )(O) que(O) o(O) endereço(O) do(O)\n",
            "\n",
            "Problema #17 (Índice Global do Token: 138521)\n",
            "  ID da Sentença: 4905\n",
            "  Token: 'do'\n",
            "  Rótulo Original: I-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: I-LEGISLACAO\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): Logo(O) ,(O) tem-se(O) que(O) o(O) artigo(B-LOCAL) 276(I-LOCAL) **do**(Original:I-LOCAL|Sugerido:I-LEGISLACAO)** Decreto(I-LOCAL) nº(I-LOCAL) 3.048/99(I-LOCAL) especificamente(O) fixa(O) o(O) dia(O) dois(O) do(O) mês(O)\n",
            "\n",
            "Problema #18 (Índice Global do Token: 181082)\n",
            "  ID da Sentença: 6176\n",
            "  Token: 'Automóvel'\n",
            "  Rótulo Original: I-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: B-LOCAL\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): -(O) São(B-LOCAL) Sebastião(I-LOCAL) ;(O) XXVII(O) -(O) SCIA(B-LOCAL) ((O) Cidade(B-LOCAL) do(I-LOCAL) **Automóvel**(Original:I-LOCAL|Sugerido:B-LOCAL)** e(O) Estrutural(B-LOCAL) )(O) ;(O) XXVIII(O) -(O) SIA(B-LOCAL) e(O) Setores(O) Complementares(O)\n",
            "\n",
            "Problema #19 (Índice Global do Token: 45418)\n",
            "  ID da Sentença: 1482\n",
            "  Token: 'Vara'\n",
            "  Rótulo Original: B-LOCAL\n",
            "  Rótulo Sugerido pelo Modelo: I-ORGANIZACAO\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): às(O) fls(O) .(O) 23-TJ(O) pelo(O) douto(O) Juiz(O) de(O) Direito(O) da(O) **Vara**(Original:B-LOCAL|Sugerido:I-ORGANIZACAO)** única(I-LOCAL) da(I-LOCAL) Comarca(I-LOCAL) de(I-LOCAL) Santa(I-LOCAL) Maria(I-LOCAL) do(I-LOCAL) Suaçuí/MG(I-LOCAL) ,(O) que(O)\n",
            "\n",
            "Problema #20 (Índice Global do Token: 9667)\n",
            "  ID da Sentença: 341\n",
            "  Token: 'período'\n",
            "  Rótulo Original: B-TEMPO\n",
            "  Rótulo Sugerido pelo Modelo: O\n",
            "  Confiança do Modelo no Rótulo Original: 0.0000\n",
            "  Contexto (±10 palavras): e(O) cinco(O) mil(O) reais(O) )(O) por(O) cada(O) mês(O) referente(O) ao(O) **período**(Original:B-TEMPO|Sugerido:O)** de(I-TEMPO) novembro(I-TEMPO) de(I-TEMPO) 2014(I-TEMPO) outubro(I-TEMPO) de(I-TEMPO) 2015(I-TEMPO) ((O) data(O) da(O)\n",
            "\n",
            "Fim da exibição dos problemas.\n"
          ]
        }
      ],
      "source": [
        "for i, problema in enumerate(problemas_para_revisao_ordenados[:min(20, num_problemas_encontrados)]):\n",
        "    print(f\"\\nProblema #{i+1} (Índice Global do Token: {problema['indice_token_global']})\")\n",
        "    print(f\"  ID da Sentença: {problema['id_sentenca']}\")\n",
        "    print(f\"  Token: '{problema['token']}'\")\n",
        "    print(f\"  Rótulo Original: {problema['rotulo_original']}\")\n",
        "    print(f\"  Rótulo Sugerido pelo Modelo: {problema['rotulo_sugerido_pelo_modelo']}\")\n",
        "    print(f\"  Confiança do Modelo no Rótulo Original: {problema['confianca_modelo_no_rotulo_original']:.4f}\")\n",
        "\n",
        "    tokens_tags_sentenca = problema['contexto_sentenca_completa'] # Lista de tuplas (token_texto, rotulo_original)\n",
        "\n",
        "    idx_primeiro_token_da_sentenca_no_dataset_global = -1\n",
        "    for idx_global, sent_id_global in enumerate(ids_sentenca):\n",
        "        if sent_id_global == problema['id_sentenca']:\n",
        "            idx_primeiro_token_da_sentenca_no_dataset_global = idx_global\n",
        "            break\n",
        "\n",
        "    posicao_token_na_sentenca = problema['indice_token_global'] - idx_primeiro_token_da_sentenca_no_dataset_global\n",
        "\n",
        "    if not (0 <= posicao_token_na_sentenca < len(tokens_tags_sentenca)) or \\\n",
        "       tokens_tags_sentenca[posicao_token_na_sentenca][0] != problema['token']:\n",
        "        for idx_sent, (tk_sent, _) in enumerate(tokens_tags_sentenca):\n",
        "            if tk_sent == problema['token']:\n",
        "                posicao_token_na_sentenca = idx_sent\n",
        "                break\n",
        "    janela_contexto = 10\n",
        "\n",
        "    inicio_ctx_ant = max(0, posicao_token_na_sentenca - janela_contexto)\n",
        "    contexto_anterior_dados = tokens_tags_sentenca[inicio_ctx_ant : posicao_token_na_sentenca]\n",
        "    contexto_anterior_formatado = [f\"{tk}({tag})\" for tk, tag in contexto_anterior_dados]\n",
        "\n",
        "    token_problematico_texto = problema['token']\n",
        "    rotulo_original_problematico = problema['rotulo_original']\n",
        "    rotulo_sugerido_problematico = problema['rotulo_sugerido_pelo_modelo']\n",
        "    token_destacado_str = f\"**{token_problematico_texto}**(Original:{rotulo_original_problematico}|Sugerido:{rotulo_sugerido_problematico})**\"\n",
        "\n",
        "    inicio_ctx_post = posicao_token_na_sentenca + 1\n",
        "    fim_ctx_post = min(len(tokens_tags_sentenca), inicio_ctx_post + janela_contexto)\n",
        "    contexto_posterior_dados = tokens_tags_sentenca[inicio_ctx_post : fim_ctx_post]\n",
        "    contexto_posterior_formatado = [f\"{tk}({tag})\" for tk, tag in contexto_posterior_dados]\n",
        "\n",
        "    partes_finais_contexto = []\n",
        "    if contexto_anterior_formatado:\n",
        "        partes_finais_contexto.append(\" \".join(contexto_anterior_formatado))\n",
        "\n",
        "    partes_finais_contexto.append(token_destacado_str)\n",
        "\n",
        "    if contexto_posterior_formatado:\n",
        "        partes_finais_contexto.append(\" \".join(contexto_posterior_formatado))\n",
        "\n",
        "    print(f\"  Contexto (±{janela_contexto} palavras): {' '.join(partes_finais_contexto)}\")\n",
        "\n",
        "print(\"\\nFim da exibição dos problemas.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesse momento analisamos a saída do nosso modelo de confident learning. Vemos que nos primeiros problemas identificados o modelo acertadamente apontou erros na anotação humano. Os problemas #1 e #2 tratam-se claramente de exemplos de legislação cadastrados erroneamente: \\*\\*artigo\\*\\*(Original:B-LOCAL|Sugerido:B-LEGISLACAO)\\*\\* 276(I-LOCAL) e  \\*\\*Autos\\*\\*(Original:B-LOCAL|Sugerido:B-JURISPRUDENCIA)\\*\\* n.º(I-LOCAL) 1002199-81.2017.8.01.0000/50000(I-LOCAL).\n",
        "\n",
        "Todavia, há exemplos em que nosso modelo se confundiu ao apontar problemas em labels originais. No problema #3, a vírgula no endereço QUADRA(B-LOCAL) 1(I-LOCAL) \\*\\* , \\*\\*(Original:I-LOCAL|Sugerido:O)** DO(I-LOCAL) SETOR(I-LOCAL) DE(I-LOCAL) INDÚSTRIAS(I-LOCAL) GRÁFICAS(I-LOCAL) deve ser, de fato, considerada parte do label LOCAL.\n",
        "\n",
        "Não obstante o exemplo de engano cometido pelo modelo, percebe-se a eficiência do modelo em identificar os labels problemáticos, atestando a eficácia da técnica aplicada.\n",
        "\n"
      ],
      "metadata": {
        "id": "Iv_Az5ghjnRh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_k9BQoRN5Tz"
      },
      "source": [
        "# Conclusão\n",
        "\n",
        "Neste notebook, aplicamos técnicas de Confident Learning utilizando a biblioteca cleanlab para detectar erros de anotação no dataset LeNER-Br, amplamente utilizado em tarefas de Reconhecimento de Entidades Nomeadas (NER) em língua portuguesa.\n",
        "\n",
        "Identificamos automaticamente diversos rótulos inconsistentes entre as anotações humanas e as previsões do modelo treinado, com base em critérios de baixa confiança. Foi possível observar que muitos dos erros apontados pelo modelo indicavam de fato falhas de rotulagem no conjunto original, como a anotação equivocada de expressões legais e nomes de jurisprudência como localidades.\n",
        "\n",
        "Ainda que alguns falsos positivos tenham sido identificados — como o caso da vírgula no endereço classificada incorretamente pelo modelo — os resultados demonstram a relevância da técnica para auditoria e refinamento de datasets anotados manualmente.\n",
        "\n",
        "Concluímos que o uso de Confident Learning representa uma abordagem eficaz para a melhoria da qualidade de conjuntos de dados anotados, sobretudo em tarefas sensíveis como o NER jurídico, onde erros de anotação podem impactar significativamente o desempenho dos modelos.\n",
        "\n",
        "Como etapa futura, recomenda-se a aplicação de técnicas de retagging automatizado ou semiautomático para corrigir os rótulos identificados como problemáticos, utilizando as previsões de maior confiança do modelo como sugestão inicial para revisão humana."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM/RdvP+AB4I9n9iRUmRvEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}